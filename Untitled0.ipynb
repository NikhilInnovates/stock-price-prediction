Code cell <bipkqB_rj_dY>
# %% [code]
!pip install alpha_vantage finnhub-python websocket-client pandas plotly scikit-learn
Execution output from Mar 6, 2025 10:49 AM
4KB
	Stream
		Collecting alpha_vantage
		  Downloading alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)
		Collecting finnhub-python
		  Downloading finnhub_python-2.4.22-py3-none-any.whl.metadata (9.0 kB)
		Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (1.8.0)
		Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)
		Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)
		Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)
		Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (3.11.13)
		Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (2.32.3)
		Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)
		Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)
		Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)
		Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)
		Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)
		Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)
		Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)
		Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)
		Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)
		Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)
		Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.4.1)
		Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.10)
		Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2.3.0)
		Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2025.1.31)
		Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (2.4.6)
		Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.3.2)
		Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (25.1.0)
		Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.5.0)
		Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (6.1.0)
		Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (0.3.0)
		Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.18.3)
		Downloading alpha_vantage-3.0.0-py3-none-any.whl (35 kB)
		Downloading finnhub_python-2.4.22-py3-none-any.whl (11 kB)
		Installing collected packages: finnhub-python, alpha_vantage
		Successfully installed alpha_vantage-3.0.0 finnhub-python-2.4.22

Code cell <dOmqqk1h8XU8>
# %% [code]
import finnhub
import websocket
import json
import time
import pandas as pd
import threading
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from alpha_vantage.timeseries import TimeSeries
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import IsolationForest
import numpy as np

Code cell <1Ir2HW-WDPUU>
# %% [code]
# Replace with your Finnhub API key
FINNHUB_API_KEY = "your key"  # <--- Paste your key here
finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)

# Replace with your Alpha Vantage API key
ALPHA_VANTAGE_API_KEY = "your key"  # <--- Paste your key here
ts = TimeSeries(key=ALPHA_VANTAGE_API_KEY, output_format="pandas")
Execution output from Mar 6, 2025 5:19 PM
1KB
	Error
		NameError
		---------------------------------------------------------------------------
		NameError                                 Traceback (most recent call last)
		<ipython-input-19-76058c9ff685> in <cell line: 0>()
		      1 # Replace with your Finnhub API key
		      2 FINNHUB_API_KEY = "your key"  # <--- Paste your key here
		----> 3 finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)
		      4 
		      5 # Replace with your Alpha Vantage API key
		
		NameError: name 'finnhub' is not defined

Code cell <rOw4KiELG5CN>
# %% [code]
print("Fetching all US stock symbols...")
try:
    symbols_data = finnhub_client.stock_symbols("US")
    stock_symbols = [s["symbol"] for s in symbols_data if s["type"] == "Common Stock"]
    print(f"Total US stocks available: {len(stock_symbols)}")

    # Limit to only 20 stocks
    stock_symbols = stock_symbols[:20]
    print(f"Selected 20 stocks: {stock_symbols}")
except Exception as e:
    print(f"Error fetching symbols: {e}")
    stock_symbols = ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "NFLX", "FB", "BRK.A", "V", "JNJ", "WMT", "PG", "MA", "DIS", "NVDA", "PYPL", "BAC", "HD", "INTC", "CSCO"]  # Fallback
Execution output from Mar 6, 2025 10:49 AM
0KB
	Stream
		Fetching all US stock symbols...
		Total US stocks available: 17464
		Selected 20 stocks: ['BGXXQ', 'SNTE', 'EXCE', 'PEMTF', 'JPM', 'FPWM', 'BOIVF', 'BKTRF', 'ALKN', 'BLUAF', 'SAMHF', 'SZCRF', 'STCC', 'KLGG', 'VSME', 'VLDX', 'UAL', 'HOFBF', 'QWTR', 'SAIDF']

Code cell <w2qSdAJtG7fV>
# %% [code]
def get_closing_price(symbol):
    try:
        quote = finnhub_client.quote(symbol)
        closing_price = quote["pc"]
        print(f"{symbol} last closed at ${closing_price:.2f}")
        return closing_price
    except Exception as e:
        print(f"Error fetching {symbol}: {e}")
        return None

Code cell <OcGu0wx5HQ6u>
# %% [code]
print("\nFetching closing prices for 20 stocks...")
closing_prices = []
for symbol in stock_symbols:
    price = get_closing_price(symbol)
    closing_prices.append(price)
Execution output from Mar 6, 2025 10:49 AM
1KB
	Stream
		Fetching closing prices for 20 stocks...
		BGXXQ last closed at $0.03
		SNTE last closed at $0.01
		EXCE last closed at $10.00
		PEMTF last closed at $0.00
		JPM last closed at $250.25
		FPWM last closed at $0.00
		BOIVF last closed at $6.03
		BKTRF last closed at $0.07
		ALKN last closed at $0.00
		BLUAF last closed at $10.32
		SAMHF last closed at $8.60
		SZCRF last closed at $25.15
		STCC last closed at $0.00
		KLGG last closed at $0.00
		VSME last closed at $1.01
		VLDX last closed at $0.19
		UAL last closed at $86.21
		HOFBF last closed at $0.10
		QWTR last closed at $0.04
		SAIDF last closed at $0.02

Code cell <lr77EC6mHf5U>
# %% [code]
from google.colab import drive
drive.mount('/content/drive')

# Create a DataFrame
df = pd.DataFrame({
    "Symbol": stock_symbols,
    "Closing Price": closing_prices
})

# Save to Google Drive
df.to_csv("/content/drive/My Drive/20_stocks_closing_prices.csv", index=False)
print("Data saved to Google Drive.")
Execution output from Mar 6, 2025 10:50 AM
0KB
	Stream
		Mounted at /content/drive
		Data saved to Google Drive.

Code cell <5tQAYB2jJ000>
# %% [code]
# Initialize a dictionary to store trade data
trade_data = {symbol: {"prices": [], "timestamps": [], "volumes": []} for symbol in stock_symbols}

def on_message(ws, message):
    data = json.loads(message)
    if "data" in data:
        for trade in data["data"]:
            symbol = trade["s"]
            price = trade["p"]
            volume = trade["v"]
            timestamp = trade["t"] / 1000

            # Store trade data
            trade_data[symbol]["prices"].append(price)
            trade_data[symbol]["timestamps"].append(timestamp)
            trade_data[symbol]["volumes"].append(volume)

            # Calculate metrics
            trade_count = len(trade_data[symbol]["prices"])
            avg_price = sum(trade_data[symbol]["prices"]) / trade_count
            total_volume = sum(trade_data[symbol]["volumes"])

            print(f"{symbol}: Trade Count = {trade_count}, Avg Price = ${avg_price:.2f}, Total Volume = {total_volume}")

def on_error(ws, error):
    print(f"Error: {error}")

def on_close(ws, close_status_code, close_msg):
    print(f"WebSocket closed: {close_status_code}, {close_msg}")

def on_open(ws):
    print("WebSocket opened")
    # Subscribe to the 20 selected stocks
    for symbol in stock_symbols:
        ws.send(json.dumps({"type": "subscribe", "symbol": symbol}))
    print(f"Subscribed to {len(stock_symbols)} stocks")

# Start WebSocket in a separate thread
ws_url = "wss://ws.finnhub.io?token=" + FINNHUB_API_KEY
ws = websocket.WebSocketApp(ws_url,
                            on_open=on_open,
                            on_message=on_message,
                            on_error=on_error,
                            on_close=on_close)

def run_websocket():
    ws.run_forever()

threading.Thread(target=run_websocket, daemon=True).start()

Code cell <-THhkb2OJ6TW>
# %% [code]
print("\nStreaming stock data for 30 seconds...")
time.sleep(30)
print("Test complete")
Execution output from Mar 6, 2025 10:51 AM
0KB
	Stream
		Streaming stock data for 30 seconds...
		Test complete

Code cell <JH5c6K-hJ_4Y>
# %% [code]
def save_metrics_to_csv():
    metrics = []
    for symbol in stock_symbols:
        trade_count = len(trade_data[symbol]["prices"])
        avg_price = sum(trade_data[symbol]["prices"]) / trade_count if trade_count > 0 else 0
        total_volume = sum(trade_data[symbol]["volumes"])
        metrics.append([symbol, trade_count, avg_price, total_volume])

    # Create a DataFrame
    metrics_df = pd.DataFrame(metrics, columns=["Symbol", "Trade Count", "Average Price", "Total Volume"])

    # Save to Google Drive
    metrics_df.to_csv("/content/drive/My Drive/20_stocks_metrics.csv", index=False)
    print("Metrics saved to Google Drive.")

save_metrics_to_csv()
Execution output from Mar 6, 2025 10:52 AM
0KB
	Stream
		Metrics saved to Google Drive.

Code cell <VtvTXq_RKS1s>
# %% [code]
def fetch_historical_data(symbol):
    try:
        # Fetch daily historical data
        data, meta_data = ts.get_daily(symbol=symbol, outputsize="compact")
        print(f"Fetched historical data for {symbol}")
        return data
    except Exception as e:
        print(f"Error fetching historical data for {symbol}: {e}")
        return None

Code cell <YNrHhXhUKdg8>
# %% [code]
# Initialize a dictionary to store historical data
historical_data = {}

# Fetch historical data for each stock
for symbol in stock_symbols:
    data = fetch_historical_data(symbol)
    if data is not None:
        historical_data[symbol] = data
Execution output from Mar 6, 2025 10:52 AM
1KB
	Stream
		Fetched historical data for BGXXQ
		Fetched historical data for SNTE
		Fetched historical data for EXCE
		Fetched historical data for PEMTF
		Fetched historical data for JPM
		Fetched historical data for FPWM
		Fetched historical data for BOIVF
		Fetched historical data for BKTRF
		JPM: Trade Count = 1, Avg Price = $248.88, Total Volume = 20
		Fetched historical data for ALKN
		Fetched historical data for BLUAF
		Fetched historical data for SAMHF
		Fetched historical data for SZCRF
		Fetched historical data for STCC
		Fetched historical data for KLGG
		Fetched historical data for VSME
		Fetched historical data for VLDX
		Fetched historical data for UAL
		Fetched historical data for HOFBF
		Fetched historical data for QWTR
		Fetched historical data for SAIDF

Code cell <y-UiTCtIKwBd>
# %% [code]
# Save historical data to CSV files
for symbol, data in historical_data.items():
    file_path = f"/content/drive/My Drive/{symbol}_historical_data.csv"
    data.to_csv(file_path)
    print(f"Saved historical data for {symbol} to {file_path}")
Execution output from Mar 6, 2025 10:52 AM
2KB
	Stream
		Saved historical data for BGXXQ to /content/drive/My Drive/BGXXQ_historical_data.csv
		Saved historical data for SNTE to /content/drive/My Drive/SNTE_historical_data.csv
		Saved historical data for EXCE to /content/drive/My Drive/EXCE_historical_data.csv
		Saved historical data for PEMTF to /content/drive/My Drive/PEMTF_historical_data.csv
		Saved historical data for JPM to /content/drive/My Drive/JPM_historical_data.csv
		Saved historical data for FPWM to /content/drive/My Drive/FPWM_historical_data.csv
		Saved historical data for BOIVF to /content/drive/My Drive/BOIVF_historical_data.csv
		Saved historical data for BKTRF to /content/drive/My Drive/BKTRF_historical_data.csv
		Saved historical data for ALKN to /content/drive/My Drive/ALKN_historical_data.csv
		Saved historical data for BLUAF to /content/drive/My Drive/BLUAF_historical_data.csv
		Saved historical data for SAMHF to /content/drive/My Drive/SAMHF_historical_data.csv
		Saved historical data for SZCRF to /content/drive/My Drive/SZCRF_historical_data.csv
		Saved historical data for STCC to /content/drive/My Drive/STCC_historical_data.csv
		Saved historical data for KLGG to /content/drive/My Drive/KLGG_historical_data.csv
		Saved historical data for VSME to /content/drive/My Drive/VSME_historical_data.csv
		Saved historical data for VLDX to /content/drive/My Drive/VLDX_historical_data.csv
		Saved historical data for UAL to /content/drive/My Drive/UAL_historical_data.csv
		Saved historical data for HOFBF to /content/drive/My Drive/HOFBF_historical_data.csv
		Saved historical data for QWTR to /content/drive/My Drive/QWTR_historical_data.csv
		Saved historical data for SAIDF to /content/drive/My Drive/SAIDF_historical_data.csv

Text cell <UxFuXlU7K5vw>
# %% [markdown]
Example

Code cell <4Cm9dLC7K68e>
# %% [code]
# Display historical data for the first stock
symbol = stock_symbols[0]
if symbol in historical_data:
    print(f"Historical data for {symbol}:")
    print(historical_data[symbol].head())
Execution output from Mar 6, 2025 10:53 AM
1KB
	Stream
		Historical data for BGXXQ:
		            1. open  2. high  3. low  4. close  5. volume
		date                                                     
		2025-03-05   0.0301   0.0475  0.0301    0.0418    98297.0
		2025-03-04   0.0302   0.0429  0.0280    0.0331    75795.0
		2025-03-03   0.0340   0.0499  0.0311    0.0332    67447.0
		2025-02-28   0.0397   0.0500  0.0275    0.0330   534135.0
		2025-02-27   0.0321   0.0399  0.0321    0.0340    71229.0

Code cell <URR4XZz1LpXA>
# %% [code]
!pip install torch
Execution output from Mar 6, 2025 10:53 AM
3KB
	Stream
		Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)
		Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)
		Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)
		Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)
		Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)
		Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)
		Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
		Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
		Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
		Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)
		Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)
		Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)
		Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)
		Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)
		Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)
		Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)
		Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
		Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
		Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)
		Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)
		Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)
		Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)

Code cell <IvXvoG2TO2vP>
# %% [code]
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import plotly.graph_objects as go

Code cell <FgF8Mm_eO85Q>
# %% [code]
class StockDataset(Dataset):
    def __init__(self, data, sequence_length=60):
        self.data = data
        self.sequence_length = sequence_length
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.scaled_data = self.scaler.fit_transform(data["4. close"].values.reshape(-1, 1))

    def __len__(self):
        return len(self.scaled_data) - self.sequence_length

    def __getitem__(self, idx):
        X = self.scaled_data[idx:idx+self.sequence_length]
        y = self.scaled_data[idx+self.sequence_length]
        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

Code cell <eUYWQGU5PYpk>
# %% [code]
# Check for missing symbols
missing_symbols = [symbol for symbol in stock_symbols if symbol not in historical_data]
print("Missing symbols:", missing_symbols)
Execution output from Mar 6, 2025 10:53 AM
0KB
	Stream
		Missing symbols: []

Code cell <aTMpPqANQGMj>
# %% [code]
# Remove missing symbols from stock_symbols
stock_symbols = [symbol for symbol in stock_symbols if symbol not in ['ARHIS', 'PTRKF']]
print("Updated stock symbols:", stock_symbols)
Execution output from Mar 6, 2025 10:53 AM
0KB
	Stream
		Updated stock symbols: ['BGXXQ', 'SNTE', 'EXCE', 'PEMTF', 'JPM', 'FPWM', 'BOIVF', 'BKTRF', 'ALKN', 'BLUAF', 'SAMHF', 'SZCRF', 'STCC', 'KLGG', 'VSME', 'VLDX', 'UAL', 'HOFBF', 'QWTR', 'SAIDF']

Code cell <BqU9rGXIQH7O>
# %% [code]
# Combine historical data of all stocks
combined_data = pd.concat([historical_data[symbol] for symbol in stock_symbols], axis=0)
print("Combined historical data shape:", combined_data.shape)
Execution output from Mar 6, 2025 10:54 AM
0KB
	Stream
		Combined historical data shape: (2000, 5)

Code cell <eKXTTURiQYYk>
# %% [code]
class StockDataset(Dataset):
    def __init__(self, data, sequence_length=60):
        """
        Args:
            data (pd.DataFrame): Combined historical data of all stocks.
            sequence_length (int): Length of the input sequence (default: 60 days).
        """
        self.data = data
        self.sequence_length = sequence_length
        self.scaler = MinMaxScaler(feature_range=(0, 1))  # Initialize the scaler
        self.scaled_data = self.scaler.fit_transform(data["4. close"].values.reshape(-1, 1))  # Scale the data

    def __len__(self):
        """
        Returns the total number of sequences in the dataset.
        """
        return len(self.scaled_data) - self.sequence_length

    def __getitem__(self, idx):
        """
        Returns a sequence of `sequence_length` days as input (X) and the next day's closing price as the target (y).
        """
        X = self.scaled_data[idx:idx+self.sequence_length]  # Input sequence
        y = self.scaled_data[idx+self.sequence_length]      # Target value
        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

Code cell <KNPscOZXQddN>
# %% [code]
# Create dataset for combined data
sequence_length = 60  # Use 60 days of historical data to predict the next day's price
combined_dataset = StockDataset(combined_data, sequence_length)
print(f"Number of sequences in the dataset: {len(combined_dataset)}")
Execution output from Mar 6, 2025 10:54 AM
0KB
	Stream
		Number of sequences in the dataset: 1940

Code cell <joyAl33uQiUN>
# %% [code]
# Create dataloader for combined data
batch_size = 32  # Number of sequences per batch
combined_dataloader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)
print(f"Number of batches in the dataloader: {len(combined_dataloader)}")
Execution output from Mar 6, 2025 10:54 AM
0KB
	Stream
		Number of batches in the dataloader: 61

Text cell <DUdmpgA6Qyc_>
# %% [markdown]
Transformer building


Code cell <wAlkd85MQ0Zu>
# %% [code]
import torch.nn as nn

class TransformerModel(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):
        """
        Args:
            input_dim (int): Dimension of the input features (e.g., 1 for closing price).
            model_dim (int): Dimension of the model's hidden states.
            num_heads (int): Number of attention heads in the Transformer.
            num_layers (int): Number of encoder and decoder layers in the Transformer.
            output_dim (int): Dimension of the output (e.g., 1 for predicting the next day's closing price).
        """
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, model_dim)  # Embedding layer
        self.transformer = nn.Transformer(
            d_model=model_dim,  # Model dimension
            nhead=num_heads,    # Number of attention heads
            num_encoder_layers=num_layers,  # Number of encoder layers
            num_decoder_layers=num_layers,  # Number of decoder layers
            dim_feedforward=2048,  # Feedforward network dimension
            dropout=0.1  # Dropout rate
        )
        self.fc = nn.Linear(model_dim, output_dim)  # Final fully connected layer

    def forward(self, src):
        """
        Forward pass of the model.
        Args:
            src (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_dim).
        Returns:
            torch.Tensor: Output tensor of shape (batch_size, output_dim).
        """
        # src shape: (batch_size, sequence_length, input_dim)
        src = self.embedding(src)  # Embed the input
        src = src.permute(1, 0, 2)  # Reshape to (sequence_length, batch_size, model_dim)
        output = self.transformer(src, src)  # Pass through the Transformer
        output = output.permute(1, 0, 2)  # Reshape back to (batch_size, sequence_length, model_dim)
        output = self.fc(output[:, -1, :])  # Use the last output for prediction
        return output

Code cell <NTxnWKnxQ-b7>
# %% [code]
# Initialize the model
input_dim = 1  # Using only the 'Close' price
model_dim = 64  # Dimension of the model's hidden states
num_heads = 4  # Number of attention heads
num_layers = 2  # Number of encoder and decoder layers
output_dim = 1  # Predicting the next day's closing price

model = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim)
print(model)
Execution output from Mar 6, 2025 10:54 AM
3KB
	Stream
		TransformerModel(
		  (embedding): Linear(in_features=1, out_features=64, bias=True)
		  (transformer): Transformer(
		    (encoder): TransformerEncoder(
		      (layers): ModuleList(
		        (0-1): 2 x TransformerEncoderLayer(
		          (self_attn): MultiheadAttention(
		            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
		          )
		          (linear1): Linear(in_features=64, out_features=2048, bias=True)
		          (dropout): Dropout(p=0.1, inplace=False)
		          (linear2): Linear(in_features=2048, out_features=64, bias=True)
		          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (dropout1): Dropout(p=0.1, inplace=False)
		          (dropout2): Dropout(p=0.1, inplace=False)
		        )
		      )
		      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		    )
		    (decoder): TransformerDecoder(
		      (layers): ModuleList(
		        (0-1): 2 x TransformerDecoderLayer(
		          (self_attn): MultiheadAttention(
		            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
		          )
		          (multihead_attn): MultiheadAttention(
		            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
		          )
		          (linear1): Linear(in_features=64, out_features=2048, bias=True)
		          (dropout): Dropout(p=0.1, inplace=False)
		          (linear2): Linear(in_features=2048, out_features=64, bias=True)
		          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (dropout1): Dropout(p=0.1, inplace=False)
		          (dropout2): Dropout(p=0.1, inplace=False)
		          (dropout3): Dropout(p=0.1, inplace=False)
		        )
		      )
		      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		    )
		  )
		  (fc): Linear(in_features=64, out_features=1, bias=True)
		)
		/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
		  warnings.warn(

Code cell <P1bSDOXJRQkk>
# %% [code]
# Define loss function and optimizer
criterion = nn.MSELoss()  # Mean Squared Error loss
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001

Code cell <vUdQX9_pRVj5>
# %% [code]
# Training loop
num_epochs = 20  # Number of epochs
for epoch in range(num_epochs):
    for X, y in combined_dataloader:
        optimizer.zero_grad()  # Clear gradients
        outputs = model(X)  # Forward pass
        loss = criterion(outputs, y)  # Compute loss
        loss.backward()  # Backward pass (compute gradients)
        optimizer.step()  # Update weights
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
Execution output from Mar 6, 2025 11:03 AM
1KB
	Stream
		Epoch [1/20], Loss: 0.0482
		JPM: Trade Count = 2, Avg Price = $248.88, Total Volume = 21
		Epoch [2/20], Loss: 0.0073
		Epoch [3/20], Loss: 0.0105
		Epoch [4/20], Loss: 0.0036
		JPM: Trade Count = 3, Avg Price = $248.88, Total Volume = 23
		Epoch [5/20], Loss: 0.0017
		Epoch [6/20], Loss: 0.0014
		Epoch [7/20], Loss: 0.0016
		Epoch [8/20], Loss: 0.0004
		Epoch [9/20], Loss: 0.0008
		Epoch [10/20], Loss: 0.0007
		Epoch [11/20], Loss: 0.0006
		JPM: Trade Count = 4, Avg Price = $248.85, Total Volume = 26
		JPM: Trade Count = 5, Avg Price = $248.83, Total Volume = 39
		Epoch [12/20], Loss: 0.0010
		Epoch [13/20], Loss: 0.0005
		Epoch [14/20], Loss: 0.0050
		JPM: Trade Count = 6, Avg Price = $248.83, Total Volume = 40
		Epoch [15/20], Loss: 0.0001
		JPM: Trade Count = 7, Avg Price = $248.83, Total Volume = 41
		Epoch [16/20], Loss: 0.0025
		JPM: Trade Count = 8, Avg Price = $248.81, Total Volume = 42
		JPM: Trade Count = 9, Avg Price = $248.81, Total Volume = 52
		JPM: Trade Count = 10, Avg Price = $248.80, Total Volume = 57
		Epoch [17/20], Loss: 0.0006
		JPM: Trade Count = 11, Avg Price = $248.79, Total Volume = 58
		JPM: Trade Count = 12, Avg Price = $248.78, Total Volume = 59
		Epoch [18/20], Loss: 0.0003
		Epoch [19/20], Loss: 0.0007
		Epoch [20/20], Loss: 0.0011

Text cell <d5TFnQkhU4Eu>
# %% [markdown]
Compute Training Loss

Code cell <3wdGJEc2UtIQ>
# %% [code]
model.eval()  # Set the model to evaluation mode
total_loss = 0
with torch.no_grad():  # Disable gradient computation
    for X, y in combined_dataloader:
        outputs = model(X)  # Forward pass
        loss = criterion(outputs, y)  # Compute loss
        total_loss += loss.item() * X.size(0)  # Accumulate loss

# Compute average loss
average_loss = total_loss / len(combined_dataset)
print(f"Training Loss (MSE): {average_loss:.4f}")
Execution output from Mar 6, 2025 11:03 AM
0KB
	Stream
		Training Loss (MSE): 0.0025

Code cell <4EC3GlnJVCyN>
# %% [code]
# Select a specific stock
symbol = stock_symbols[1]  # First stock in the list
data = historical_data[symbol]  # Historical data for the specific stock

# Prepare the data for prediction
def prepare_data_for_prediction(data, scaler, sequence_length=60):
    last_sequence = data["4. close"].values[-sequence_length:]  # Last 60 days of data
    last_sequence_scaled = scaler.transform(last_sequence.reshape(-1, 1))  # Scale the data
    return torch.tensor(last_sequence_scaled, dtype=torch.float32).unsqueeze(0)  # Add batch dimension

# Prepare the input sequence
X_input = prepare_data_for_prediction(data, combined_dataset.scaler)

Text cell <lYEPZ9xYVHR6>
# %% [markdown]
Prediction

Code cell <ORW35ezpVIXu>
# %% [code]
# Function to predict future prices
def predict_future_prices(model, X_input, scaler, future_days=30):
    model.eval()  # Set the model to evaluation mode
    predictions = []
    for _ in range(future_days):
        with torch.no_grad():
            predicted_price_scaled = model(X_input).item()  # Predict the next day's price
        predictions.append(predicted_price_scaled)

        # Update the input sequence
        X_input = torch.cat([X_input[:, 1:, :], torch.tensor(predicted_price_scaled).reshape(1, 1, 1)], dim=1)

    # Inverse transform the predictions
    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))
    return predictions

# Predict future prices
future_prices = predict_future_prices(model, X_input, combined_dataset.scaler)
print(f"Predicted future prices for {symbol}:")
print(future_prices)
Execution output from Mar 6, 2025 11:04 AM
1KB
	Stream
		Predicted future prices for SNTE:
		[[ -8.62550938]
		 [-10.35867992]
		 [-10.66723502]
		 [-10.74245069]
		 [-10.78135067]
		 [-10.814813  ]
		 [-10.84759954]
		 [-10.8805133 ]
		 [-10.91367528]
		 [-10.94710841]
		 [-10.98082938]
		 [-11.01477561]
		 [-11.04917446]
		 [-11.08395084]
		 [-11.11908806]
		 [-11.15455066]
		 [-11.19031778]
		 [-11.2264687 ]
		 [-11.2626509 ]
		 [-11.29895824]
		 [-11.33565146]
		 [-11.37252823]
		 [-11.40968032]
		 [-11.44715363]
		 [-11.48505035]
		 [-11.52324325]
		 [-11.56223918]
		 [-11.60160638]
		 [-11.64204568]
		 [-11.68320874]]

Code cell <tzk1ntbVWkYy>
# %% [code]
# Plot historical and predicted prices
fig = go.Figure()
fig.add_trace(go.Scatter(x=data.index, y=data["4. close"], mode="lines", name="Historical Prices"))
fig.add_trace(go.Scatter(x=pd.date_range(data.index[-1], periods=len(future_prices) + 1)[1:],
                         y=future_prices.flatten(), mode="lines", name="Predicted Prices"))
fig.update_layout(title=f"{symbol} Stock Price Prediction (Transformer)", xaxis_title="Date", yaxis_title="Price")
fig.show()
Execution output from Mar 6, 2025 11:04 AM
15KB

Code cell <zL16S1WyW0zQ>
# %% [code]
!pip install fastapi uvicorn
Execution output from Mar 6, 2025 10:16 AM
2KB
	Stream
		Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.11)
		Collecting uvicorn
		  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)
		Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.0)
		Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)
		Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)
		Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)
		Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)
		Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)
		Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)
		Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)
		Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)
		Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)
		Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)
		[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m62.3/62.3 kB[0m [31m1.9 MB/s[0m eta [36m0:00:00[0m
		[?25hInstalling collected packages: uvicorn
		Successfully installed uvicorn-0.34.0

Code cell <MjRyrJ8HW4n5>
# %% [code]


# Save the model
model_path = "transformer_stock_model.pth"
torch.save(model.state_dict(), model_path)
print(f"Model saved to {model_path}")
Execution output from Mar 6, 2025 11:04 AM
0KB
	Stream
		Model saved to transformer_stock_model.pth

Code cell <1Ul1waEQYjvc>
# %% [code]
# Load the saved model
model = TransformerModel(input_dim=1, model_dim=64, num_heads=4, num_layers=2, output_dim=1)
model.load_state_dict(torch.load("transformer_stock_model.pth", weights_only=True))
model.eval()  # Set the model to evaluation mode
print("Model loaded successfully.")
Execution output from Mar 6, 2025 11:04 AM
0KB
	Stream
		Model loaded successfully.
		/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning:
		
		enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)

Code cell <pseGgcMaoMX8>
# %% [code]
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import joblib

# Example: Fit the scaler on training data
training_data = np.array([100, 101, 102, 103, 104]).reshape(-1, 1)  # Replace with your actual training data
scaler = MinMaxScaler(feature_range=(0, 1))
scaler.fit(training_data)

# Save the scaler
joblib.dump(scaler, "scaler.save")
Execution output from Mar 6, 2025 11:04 AM
0KB
	text/plain
		['scaler.save']

Code cell <CBCSyJcRouDL>
# %% [code]
class TransformerModel(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, model_dim)  # [seq_len, input_dim] -> [seq_len, model_dim]
        self.transformer = nn.Transformer(
            d_model=model_dim,
            nhead=num_heads,
            num_encoder_layers=num_layers,
            num_decoder_layers=num_layers,
            dim_feedforward=2048,
            dropout=0.1,
            batch_first=True  # Key difference!
        )
        self.fc = nn.Linear(model_dim, output_dim)  # [model_dim] -> [output_dim]

    def forward(self, src):
        src = self.embedding(src)  # [batch, seq_len, model_dim]
        output = self.transformer(src, src)  # [batch, seq_len, model_dim]
        output = self.fc(output[:, -1, :])  # [batch, model_dim] -> [batch, output_dim]
        return output

# Initialize the model
model = TransformerModel(input_dim=1, model_dim=64, num_heads=4, num_layers=2, output_dim=1)
model.load_state_dict(torch.load("transformer_stock_model.pth", weights_only=True))  # Use weights_only=True for security
model.eval()
Execution output from Mar 6, 2025 11:31 AM
2KB
	text/plain
		TransformerModel(
		  (embedding): Linear(in_features=1, out_features=64, bias=True)
		  (transformer): Transformer(
		    (encoder): TransformerEncoder(
		      (layers): ModuleList(
		        (0-1): 2 x TransformerEncoderLayer(
		          (self_attn): MultiheadAttention(
		            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
		          )
		          (linear1): Linear(in_features=64, out_features=2048, bias=True)
		          (dropout): Dropout(p=0.1, inplace=False)
		          (linear2): Linear(in_features=2048, out_features=64, bias=True)
		          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (dropout1): Dropout(p=0.1, inplace=False)
		          (dropout2): Dropout(p=0.1, inplace=False)
		        )
		      )
		      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		    )
		    (decoder): TransformerDecoder(
		      (layers): ModuleList(
		        (0-1): 2 x TransformerDecoderLayer(
		          (self_attn): MultiheadAttention(
		            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
		          )
		          (multihead_attn): MultiheadAttention(
		            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
		          )
		          (linear1): Linear(in_features=64, out_features=2048, bias=True)
		          (dropout): Dropout(p=0.1, inplace=False)
		          (linear2): Linear(in_features=2048, out_features=64, bias=True)
		          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		          (dropout1): Dropout(p=0.1, inplace=False)
		          (dropout2): Dropout(p=0.1, inplace=False)
		          (dropout3): Dropout(p=0.1, inplace=False)
		        )
		      )
		      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
		    )
		  )
		  (fc): Linear(in_features=64, out_features=1, bias=True)
		)

Code cell <miAchL0BwU0Y>
# %% [code]
import joblib

# Load the scaler
scaler = joblib.load("scaler.save")

Code cell <97SE94IIL8fG>
# %% [code]
%%writefile main.py
import torch
import joblib
import numpy as np
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

app = FastAPI()

MODEL_PATH = "transformer_stock_model.pth"
SCALER_PATH = "scaler.save"

class TransformerModel(torch.nn.Module):
    def __init__(self, input_dim=1, model_dim=64, num_heads=4, num_layers=2, output_dim=1):
        super(TransformerModel, self).__init__()
        self.embedding = torch.nn.Linear(input_dim, model_dim)
        self.transformer = torch.nn.Transformer(
            d_model=model_dim,
            nhead=num_heads,
            num_encoder_layers=num_layers,
            num_decoder_layers=num_layers,
            dim_feedforward=2048,
            dropout=0.1,
            batch_first=True
        )
        self.fc = torch.nn.Linear(model_dim, output_dim)

    def forward(self, src):
        src = self.embedding(src)
        output = self.transformer(src, src)
        output = self.fc(output[:, -1, :])
        return output

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TransformerModel(input_dim=1, model_dim=64, num_heads=4, num_layers=2, output_dim=1)
model.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))
model.to(device)
model.eval()

try:
    scaler = joblib.load(SCALER_PATH)
    print(f"Scaler loaded: {scaler}")
except Exception as e:
    print(f"Error loading scaler: {e}")
    scaler = None

class PredictionInput(BaseModel):
    data: list[float]

@app.post("/predict")
async def predict(input_data: PredictionInput):
    if scaler is None:
        return {"error": "Scaler not loaded"}
    try:
        print(f"Received data: {input_data.data}")
        input_array = np.array(input_data.data).reshape(-1, 1)
        print(f"Input array shape: {input_array.shape}")
        scaled_input = scaler.transform(input_array)
        print(f"Scaled input: {scaled_input}")
        tensor_input = torch.tensor(scaled_input, dtype=torch.float32).to(device).unsqueeze(0)
        print(f"Tensor input shape: {tensor_input.shape}")
        with torch.no_grad():
            pred = model(tensor_input)
            print(f"Prediction shape: {pred.shape}, Prediction: {pred}")
            pred_value = pred[0, 0].item()
        unscaled_pred = scaler.inverse_transform([[pred_value]])[0][0]
        print(f"Unscaled prediction: {unscaled_pred}")
        return {"prediction": unscaled_pred}
    except Exception as e:
        return {"error": str(e)}

@app.get("/")
async def root():
    return {"message": "FastAPI app is running"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8003)
Execution output from Mar 6, 2025 2:37 PM
0KB
	Stream
		Writing main.py

Code cell <OiLx8n2HMCW5>
# %% [code]
import subprocess

# Start the FastAPI server in the background
process = subprocess.Popen(["python", "main.py"])

# Print the process ID (PID) for reference
print("FastAPI server started with PID:", process.pid)
Execution output from Mar 6, 2025 2:36 PM
0KB
	Stream
		FastAPI server started with PID: 66072

Code cell <a0ynb_Dkr0t4>
# %% [code]
!lsof -i :8003
Execution output from Mar 6, 2025 2:40 PM
0KB
	Stream
		JPM: Trade Count = 902, Avg Price = $248.92, Total Volume = 15449
		COMMAND   PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME
		python3 48617 root    6u  IPv4 1401131      0t0  TCP *:8003 (LISTEN)

Code cell <00TeL_9Lr43_>
# %% [code]
!curl http://localhost:8003/
Execution output from Mar 6, 2025 2:40 PM
0KB
	Stream
		{"message":"FastAPI app is running"}

Code cell <QpFuCU0tr817>
# %% [code]
!curl -X POST "http://localhost:8003/predict" -H "Content-Type: application/json" -d '{"data": [100, 101, 102, 103, 104]}'
Execution output from Mar 6, 2025 2:40 PM
0KB
	Stream
		{"prediction":103.91549873352051}

Code cell <_EztoMRasBl1>
# %% [code]
from pyngrok import ngrok

# List all active tunnels
tunnels = ngrok.get_tunnels()
print("Active Tunnels:", tunnels)

# Terminate all active tunnels
for tunnel in tunnels:
    ngrok.disconnect(tunnel.public_url)
    print(f"Terminated tunnel: {tunnel.public_url}")
Execution output from Mar 6, 2025 2:50 PM
1KB
	Stream
		WARNING:pyngrok.process.ngrok:t=2025-03-06T13:50:30+0000 lvl=warn msg="Stopping forwarder" name=http-8003-975e5553-d755-4be9-b788-43d1ecf3bfba acceptErr="failed to accept connection: Listener closed"
		Active Tunnels: [<NgrokTunnel: "https://d7fe-34-81-138-181.ngrok-free.app" -> "http://localhost:8003">, <NgrokTunnel: "https://ddb9-34-81-138-181.ngrok-free.app" -> "http://localhost:8003">, <NgrokTunnel: "https://8374-34-81-138-181.ngrok-free.app" -> "http://localhost:8501">]
		Terminated tunnel: https://d7fe-34-81-138-181.ngrok-free.app
		WARNING:pyngrok.process.ngrok:t=2025-03-06T13:50:30+0000 lvl=warn msg="Stopping forwarder" name=http-8003-ca1073c4-96b8-4c60-878e-aa265b692d6b acceptErr="failed to accept connection: Listener closed"
		WARNING:pyngrok.process.ngrok:t=2025-03-06T13:50:30+0000 lvl=warn msg="Stopping forwarder" name=http-8501-970afb9d-649b-46a4-be85-c6bee25b1870 acceptErr="failed to accept connection: Listener closed"
		Terminated tunnel: https://ddb9-34-81-138-181.ngrok-free.app
		Terminated tunnel: https://8374-34-81-138-181.ngrok-free.app

Code cell <MpmyxN6auphG>
# %% [code]
# Start ngrok tunnel for FastAPI (port 8003)
fastapi_url = ngrok.connect(8003)
print("FastAPI Public URL:", fastapi_url)
Execution output from Mar 6, 2025 2:52 PM
0KB
	Stream
		FastAPI Public URL: NgrokTunnel: "https://c939-34-81-138-181.ngrok-free.app" -> "http://localhost:8003"

Code cell <4NSXw2pguzGY>
# %% [code]
!curl -X POST "https://c939-34-81-138-181.ngrok-free.app/predict" -H "Content-Type: application/json" -d '{"data": [100, 101, 102, 103, 104]}'
Execution output from Mar 6, 2025 2:53 PM
0KB
	Stream
		{"prediction":103.91549873352051}

Code cell <3PyrJvnmu6PU>
# %% [code]
# Start ngrok tunnel for Streamlit (port 8501)
streamlit_url = ngrok.connect(8501)
print("Streamlit Public URL:", streamlit_url)
Execution output from Mar 6, 2025 2:53 PM
0KB
	Stream
		Streamlit Public URL: NgrokTunnel: "https://241c-34-81-138-181.ngrok-free.app" -> "http://localhost:8501"

Code cell <acfWLMGEvcvY>
# %% [code]
from pyngrok import ngrok

# List all active tunnels
tunnels = ngrok.get_tunnels()
print("Active Tunnels:", tunnels)
Execution output from Mar 6, 2025 2:55 PM
0KB
	Stream
		Active Tunnels: [<NgrokTunnel: "https://241c-34-81-138-181.ngrok-free.app" -> "http://localhost:8501">, <NgrokTunnel: "https://c939-34-81-138-181.ngrok-free.app" -> "http://localhost:8003">]

Code cell <yxan_JvBwoZb>
# %% [code]
!curl -X POST "https://c939-34-81-138-181.ngrok-free.app/predict" -H "Content-Type: application/json" -d '{"data": [100, 101, 102, 103, 104]}'
Execution output from Mar 6, 2025 3:01 PM
0KB
	Stream
		{"prediction":103.91549873352051}

Code cell <Ma6rPcgLvJxf>
# %% [code]
!nohup streamlit run --server.port 8501 app.py > streamlit.log 2>&1 &

# Wait for Streamlit to start
import time
time.sleep(10)

# Print the ngrok link
print(f"Access your Streamlit app at: {streamlit_url}")
Execution output from Mar 6, 2025 3:11 PM
0KB
	Stream
		UAL: Trade Count = 1185, Avg Price = $89.66, Total Volume = 631942
		UAL: Trade Count = 1186, Avg Price = $89.66, Total Volume = 632012
		Access your Streamlit app at: NgrokTunnel: "https://241c-34-81-138-181.ngrok-free.app" -> "http://localhost:8501"

Code cell <1kwPo8Eew2-x>
# %% [code]
from pyngrok import ngrok

# List all active tunnels
tunnels = ngrok.get_tunnels()
print("Active Tunnels:", tunnels)
Execution output from Mar 6, 2025 3:05 PM
0KB
	Stream
		Active Tunnels: [<NgrokTunnel: "https://241c-34-81-138-181.ngrok-free.app" -> "http://localhost:8501">, <NgrokTunnel: "https://c939-34-81-138-181.ngrok-free.app" -> "http://localhost:8003">]


